from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import numpy as np
import math, time
#import tempfile
#temp_dir = tempfile.mkdtemp()

options = Options()
options.add_argument('--window-size=700,1000')
options.add_argument('--window-position=-7,0')
#options.add_argument(f"--disk-cache-dir={temp_dir}")
options.add_experimental_option("detach", True)


search = input('검색어를 입력하세요.')


URL = 'https://korean.visitkorea.or.kr/search/search_list.do?keyword='+search
driver = webdriver.Chrome(options=options)
driver.get(URL)
#print("완료")
#time.sleep(np.random.randint(2,3))


# id => #
# class => .
# tag => 
# 바로 자식 => >
a=driver.find_element(By.CSS_SELECTOR,"#s_recommend > .more_view > a")
a.click()


a = driver.find_element(By.CSS_SELECTOR,"#s_recommend > .more_view > a")
a.click()


b = driver.find_elements(By.XPATH,"//*[@id='search_result']/ul/li[*]/div[1]/div[1]/a")
for i in b :
    print(i.text)
    print("")


b=driver.find_elements(By.CSS_SELECTOR,"#search_result .common_list .tit")
for i,d in enumerate(b,1):
    print(i,d.text)


for j in range(len(b)):
    print(b[j].text)


a = driver.find_elements(By.XPATH, '//*[@id="search_result"]/ul/li[*]/div[1]/div[1]/a')
for i, d in enumerate(a,1):
    print(i,d.text)





page_number = driver.find_element(By.CSS_SELECTOR,"#page_box [id='2']")
page_number.click()
print("완료")


driver.find_element(By.CSS_SELECTOR,"#page_box [id='2']").click()


cnt = int(input('스크래핑 건수'))
cnt


page_cnt=math.ceil(cnt/10)
print(page_cnt)


no = 0
for i in range(1,page_cnt+1):
    b=driver.find_elements(By.CSS_SELECTOR,"#search_result .common_list .tit")
    for j in b:
        no +=1
        if no > cnt :
            break
        print(no,j.text)
    if i >= page_cnt :
        break
    button = driver.find_element(By.CSS_SELECTOR,f"#page_box [id='{i+1}']")
    driver.execute_script("arguments[0].click();",button)
    time.sleep(2)




















print(temp_dir)























driver.find_element(By.CSS_SELECTOR,"#s_recommend > .more_view > a").click()
time.sleep(random.randint(2,3))


l = driver.find_elements(By.CSS_SELECTOR,"#search_result")
for i in l :
    print(i)
    print("")


list_a = driver.find_elements(By.CSS_SELECTOR,"#search_result .common_list li .cont .tit a")
for i in list_a :
    print(i.text)
    print("")


l = driver.find_elements(By.XPATH,"/html/body/div[3]/div/div[1]/div[13]/ul/li[*]/div[1]/div[1]/a")
for i in l : 
    print(i.text)
    print(" ")


for i, title in enumerate(l,1):
#     print(i, title.text)
    print(i, title.text)
    print(" ")


page2 = driver.find_element(By.CSS_SELECTOR,"#page_box [id='2']")
page2.text


cnt = input('글 수')
total = int(driver.find_element(By.ID,"totalCnt").text)
try :
    cnt=int(cnt)
except :
    cnt= total
if cnt > total:
    cnt = total

print(cnt)

    


page_count=math.ceil(cnt/10)
page_count


page_count


import urllib.request as ur
ur.urlretrieve?


no = 1
img_data = {}
for i in range(1, page_count+1):
    if no > cnt : 
        break
    page2 = driver.find_element(By.CSS_SELECTOR,f"#page_box [id='{i}']")
    driver.execute_script('arguments[0].click()',page2)
    time.sleep(2)
    
    a = driver.find_elements(By.XPATH, '//*[@id="search_result"]/ul/li[*]/div[1]/div[1]/a')
    for d in range(len(a)):
        if no > cnt : 
            break
        a = driver.find_elements(By.XPATH, '//*[@id="search_result"]/ul/li[*]/div[1]/div[1]/a')
        
        #print(no,a[d].text)
        driver.execute_script('arguments[0].click();',a[d])
        time.sleep(4)
        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')
        top_title = driver.find_element(By.ID,"topTitle").text
        #print(top_title)
        
        con=" ".join(map(lambda i: i.text,driver.find_elements(By.CSS_SELECTOR,'.txt_p , .ar_title')))
        #print(con) 

        imgs=[]
        for j,el in enumerate(driver.find_elements(By.CSS_SELECTOR,".box_txtPhoto img"),1):
            src = el.get_attribute('src')
            imgs.append(src)
            #urllib.request.urlretrieve(src,f"output/img/t{d}_{j}.jpg")
        
        img_data[no]=imgs
        driver.back()
        time.sleep(2)
        

        
        no += 1
    
    
for img in img_data : 
    for i, src in enumerate(img_data[img]):
        urllib.request.urlretrieve(src,f"output/img/t{img}_{i}.jpg")


list(map(lambda a : a*2,[2]))


urllib.request.urlretrieve?



